# autonomous-lunar-lander
Deep reinforcement learning agent trained with PPO in Stable-Baselines3 to control OpenAI Gym’s LunarLander-v3 environment. Achieves >240 reward and 98+% landing success vs heuristic baseline.

This repository implements a deep-reinforcement-learning controller for OpenAI Gym’s LunarLander-v3. The project uses Stable-Baselines3’s PPO to learn landing policies from raw observations and adds lightweight wrappers to track fuel usage and touchdown error. Training is run with the default MLP policy (two 64-unit hidden layers) and standard PPO settings such as a 0.2 clip range, 0.0003 learning rate, 2048 rollout steps, and an entropy bonus of 0.005, with DummyVecEnv, VecMonitor, and VecNormalize employed for stability and logging. The baseline policy is trained for 500,000 timesteps and checkpoints, metrics, and normalisation statistics are saved for reproducibility. 

Against a simple rule-based controller, the learned policy performs substantially better. The heuristic agent succeeds only about five percent of the time and burns roughly five times more fuel, whereas the PPO baseline consistently surpasses an average reward of 240 and achieves a one-hundred-percent landing success rate over 10,000 evaluation episodes while using about 100 units of fuel. These results illustrate how hand-crafted rules struggle with drift, descent, and attitude coupling that PPO learns to manage reliably. 

I then probe robustness and shaping. A naive per-step fuel penalty tied to main-engine use, coupled with longer training, a decaying learning rate, and a tighter clip range, degraded learning rather than improving efficiency: rewards hovered near 100 and fuel use increased to around 180, consistent with the penalty introducing variance and destabilising advantage estimates. Domain shifts were more promising. Randomising the pad position, while shifting observations so the pad appears centered, eventually recovered to the 200-reward threshold but remained below the fixed-pad baseline; randomising the lander’s initial horizontal position reached roughly 220 reward with about 150 units of fuel, likely because the pad-shift occasionally placed the target over sloped terrain. 

Finally, I sweep the entropy coefficient and find a clear “Goldilocks” level: 0.01 delivers the most reliable gains, with rewards around 250 and mean fuel near 95, outperforming the nominal 0.005 setting in this setup. Overall, PPO proves strong on the default task, while the experiments highlight that poorly tuned fuel penalties can harm learning and that generalisation depends on the nature of the shift. Future work includes longer training runs in the three-to-five-million-step range, more principled shaping that blends proximity bonuses with end-of-episode fuel costs, and additional dynamics such as gravity or wind to further stress-test robustness. 
