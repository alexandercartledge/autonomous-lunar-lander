{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348ee9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: wrappers, callback, train & eval functions\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize, VecMonitor\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.logger import TensorBoardOutputFormat\n",
    "from stable_baselines3.common.utils import get_linear_fn\n",
    "from statistics import mean, stdev\n",
    "\n",
    "# Random-start wrapper\n",
    "class RandomStartWrapper(gym.Wrapper):\n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = super().reset(**kwargs)\n",
    "        shift = self.np_random.uniform(-0.4, 0.4)\n",
    "        uw = self.unwrapped\n",
    "        uw.lander.position = (uw.lander.position.x + shift, uw.lander.position.y)\n",
    "        obs = obs.copy(); obs[0] += shift\n",
    "        return obs, info\n",
    "    def step(self, action):\n",
    "        return super().step(action)\n",
    "\n",
    "# LunarWrapper for fuel & landing error\n",
    "class LunarWrapper(gym.Wrapper):\n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = super().reset(**kwargs)\n",
    "        return obs, info\n",
    "    def step(self, action):\n",
    "        obs, reward, term, trunc, info = super().step(action)\n",
    "        info[\"fuel_used\"]   = 1.0 if action==2 else 0.0\n",
    "        info[\"landing_err\"] = np.linalg.norm(obs[0:2])\n",
    "        return obs, reward, term, trunc, info\n",
    "\n",
    "# corrected callback: no configure() call\n",
    "class MetricsCallback(BaseCallback):\n",
    "    def _on_step(self) -> bool:\n",
    "        for info in self.locals[\"infos\"]:\n",
    "            if \"episode\" in info:\n",
    "                self.logger.record(\"custom/fuel_used\",   info[\"fuel_used\"])\n",
    "                self.logger.record(\"custom/landing_err\", info[\"landing_err\"])\n",
    "        return True\n",
    "\n",
    "def make_env():\n",
    "    e = gym.make(\"LunarLander-v3\")\n",
    "    e = RandomStartWrapper(e)\n",
    "    e = LunarWrapper(e)\n",
    "    return e\n",
    "\n",
    "def train_one(ent_coef, timesteps=200_000):\n",
    "    env = DummyVecEnv([make_env])\n",
    "    env = VecMonitor(env)\n",
    "    env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.0)\n",
    "    model = PPO(\n",
    "        policy=\"MlpPolicy\",\n",
    "        env=env,\n",
    "        verbose=0,\n",
    "        n_steps=2048,\n",
    "        batch_size=64,\n",
    "        learning_rate=get_linear_fn(5e-4,1e-5,0.1),\n",
    "        clip_range=0.2,\n",
    "        ent_coef=ent_coef,\n",
    "        tensorboard_log=\"./ppo_lunar_tb\",\n",
    "    )\n",
    "    model.learn(total_timesteps=timesteps, callback=MetricsCallback())\n",
    "    return model, env\n",
    "\n",
    "def evaluate(model, env, n_episodes=20):\n",
    "    rewards, fuels = [], []\n",
    "    for _ in range(n_episodes):\n",
    "        obs = env.reset(); done=False; r_sum=0; f_sum=0\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, dones, infos = env.step(action)\n",
    "            r_sum += reward[0]\n",
    "            f_sum += infos[0][\"fuel_used\"]\n",
    "            done = dones[0]\n",
    "        rewards.append(r_sum); fuels.append(f_sum)\n",
    "    return mean(rewards), mean(fuels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7974cb",
   "metadata": {},
   "source": [
    "I define two wrappers—one to randomize the start-x each episode, one to log fuel and landing error—then build helper functions train_one and evaluate. This sets us up to easily train PPO with different entropy coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8c762c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training with ent_coef = 0.0 ===\n",
      "ent_coef=0.0 → mean reward -79.1, mean fuel 573.2\n",
      "\n",
      "=== Training with ent_coef = 0.005 ===\n",
      "ent_coef=0.005 → mean reward -98.0, mean fuel 571.0\n",
      "\n",
      "=== Training with ent_coef = 0.01 ===\n",
      "ent_coef=0.01 → mean reward -28.3, mean fuel 571.9\n",
      "\n",
      "=== Training with ent_coef = 0.02 ===\n",
      "ent_coef=0.02 → mean reward -42.9, mean fuel 574.5\n",
      "\n",
      "=== Training with ent_coef = 0.05 ===\n",
      "ent_coef=0.05 → mean reward -42.4, mean fuel 574.8\n",
      "\n",
      "Best ent_coef: 0.01 with reward -28.3, fuel 571.9\n",
      "→ saved sweep metrics to 06_entropy_sweep_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: entropy sweep (and dump CSV)\n",
    "\n",
    "import pandas as pd    # ← new\n",
    "\n",
    "ent_values = [0.0, 0.005, 0.01, 0.02, 0.05]\n",
    "results = []\n",
    "\n",
    "for ent in ent_values:\n",
    "    print(f\"\\n=== Training with ent_coef = {ent} ===\")\n",
    "    model, env = train_one(ent, timesteps=200_000)\n",
    "    env.training = False\n",
    "    env.norm_reward = False\n",
    "    mean_r, mean_f = evaluate(model, env, n_episodes=20)\n",
    "    print(f\"ent_coef={ent} → mean reward {mean_r:.1f}, mean fuel {mean_f:.1f}\")\n",
    "    results.append((ent, mean_r, mean_f))\n",
    "\n",
    "# pick best by highest reward\n",
    "best = max(results, key=lambda x: x[1])\n",
    "print(f\"\\nBest ent_coef: {best[0]} with reward {best[1]:.1f}, fuel {best[2]:.1f}\")\n",
    "best_ent = best[0]\n",
    "\n",
    "# write out the sweep metrics for plotting\n",
    "df = pd.DataFrame(results, columns=[\"ent_coef\",\"mean_reward\",\"mean_fuel\"])\n",
    "df.to_csv(\"06_entropy_sweep_metrics.csv\", index=False)\n",
    "print(\"→ saved sweep metrics to 06_entropy_sweep_metrics.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a87a27e",
   "metadata": {},
   "source": [
    "Loop over ent_coef values, train each for 200 K steps, freeze normalization, and evaluate 20 episodes apiece. I then pick the coefficient that gave the highest average reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "256b7307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-training final model with ent_coef = 0.01\n",
      "Final performance: reward 236.9, fuel 106.0\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: playback & final eval of best ent_coef\n",
    "\n",
    "print(f\"Re-training final model with ent_coef = {best_ent}\")\n",
    "final_model, final_env = train_one(best_ent, timesteps=1_500_000)\n",
    "\n",
    "# playback\n",
    "import time\n",
    "from stable_baselines3.common.vec_env import VecNormalize, VecMonitor\n",
    "\n",
    "def make_play_env():\n",
    "    e = gym.make(\"LunarLander-v3\", render_mode=\"human\")\n",
    "    e = RandomStartWrapper(e)\n",
    "    e = LunarWrapper(e)\n",
    "    return e\n",
    "\n",
    "play_env = DummyVecEnv([make_play_env])\n",
    "play_env = VecMonitor(play_env)\n",
    "play_env = VecNormalize.load(\"ppo_vecnormalize.pkl\", play_env)\n",
    "play_env.training=False; play_env.norm_reward=False\n",
    "\n",
    "obs = play_env.reset()\n",
    "for i in range(300):\n",
    "    action, i = final_model.predict(obs, deterministic=True)\n",
    "    obs, i, dones, i = play_env.step(action)\n",
    "    play_env.render()\n",
    "    time.sleep(0.02)\n",
    "    if dones[0]: break\n",
    "play_env.close()\n",
    "\n",
    "# final evaluation\n",
    "final_env.training=False; final_env.norm_reward=False\n",
    "fr, ff = evaluate(final_model, final_env, n_episodes=50)\n",
    "print(f\"Final performance: reward {fr:.1f}, fuel {ff:.1f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5423667c",
   "metadata": {},
   "source": [
    "Take the best entropy coefficient, retrain PPO for 1.5 M steps, watch it land in human-render mode, and finally run 50 evaluation episodes to print out the mean reward and fuel usage."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ll_env)",
   "language": "python",
   "name": "ll_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
